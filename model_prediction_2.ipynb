{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score,f1_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data_cleaned.csv\")\n",
    "data.drop(columns=['INCDTTM','year','day','month'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "       ADDRTYPE  SEVERITYCODE COLLISIONTYPE  \\\n0  Intersection             2    Pedestrian   \n1  Intersection             2        Angles   \n2         Block             1     Sideswipe   \n3  Intersection             1     Left Turn   \n4         Block             1         Other   \n\n                              JUNCTIONTYPE  UNDERINFL  WEATHER ROADCOND  \\\n0   At Intersection (intersection related)          0    Clear      Dry   \n1   At Intersection (intersection related)          0  Raining      Wet   \n2  Mid-Block (not related to intersection)          0    Clear      Dry   \n3   At Intersection (intersection related)          0  Raining      Wet   \n4  Mid-Block (not related to intersection)          0    Clear      Dry   \n\n                 LIGHTCOND PEDROWNOTGRNT  SPEEDING HITPARKEDCAR  \n0                 Daylight             Y         0            N  \n1  Dark - Street Lights On             Y         0            N  \n2                 Daylight             Y         0            N  \n3  Dark - Street Lights On             Y         0            N  \n4  Dark - Street Lights On             Y         0            N  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ADDRTYPE</th>\n      <th>SEVERITYCODE</th>\n      <th>COLLISIONTYPE</th>\n      <th>JUNCTIONTYPE</th>\n      <th>UNDERINFL</th>\n      <th>WEATHER</th>\n      <th>ROADCOND</th>\n      <th>LIGHTCOND</th>\n      <th>PEDROWNOTGRNT</th>\n      <th>SPEEDING</th>\n      <th>HITPARKEDCAR</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Intersection</td>\n      <td>2</td>\n      <td>Pedestrian</td>\n      <td>At Intersection (intersection related)</td>\n      <td>0</td>\n      <td>Clear</td>\n      <td>Dry</td>\n      <td>Daylight</td>\n      <td>Y</td>\n      <td>0</td>\n      <td>N</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Intersection</td>\n      <td>2</td>\n      <td>Angles</td>\n      <td>At Intersection (intersection related)</td>\n      <td>0</td>\n      <td>Raining</td>\n      <td>Wet</td>\n      <td>Dark - Street Lights On</td>\n      <td>Y</td>\n      <td>0</td>\n      <td>N</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Block</td>\n      <td>1</td>\n      <td>Sideswipe</td>\n      <td>Mid-Block (not related to intersection)</td>\n      <td>0</td>\n      <td>Clear</td>\n      <td>Dry</td>\n      <td>Daylight</td>\n      <td>Y</td>\n      <td>0</td>\n      <td>N</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Intersection</td>\n      <td>1</td>\n      <td>Left Turn</td>\n      <td>At Intersection (intersection related)</td>\n      <td>0</td>\n      <td>Raining</td>\n      <td>Wet</td>\n      <td>Dark - Street Lights On</td>\n      <td>Y</td>\n      <td>0</td>\n      <td>N</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Block</td>\n      <td>1</td>\n      <td>Other</td>\n      <td>Mid-Block (not related to intersection)</td>\n      <td>0</td>\n      <td>Clear</td>\n      <td>Dry</td>\n      <td>Dark - Street Lights On</td>\n      <td>Y</td>\n      <td>0</td>\n      <td>N</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1    137485\n2     58698\n3      3098\n4       349\nName: SEVERITYCODE, dtype: int64"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "data['SEVERITYCODE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "ADDRTYPE         object\nSEVERITYCODE      int64\nCOLLISIONTYPE    object\nJUNCTIONTYPE     object\nUNDERINFL         int64\nWEATHER          object\nROADCOND         object\nLIGHTCOND        object\nPEDROWNOTGRNT    object\nSPEEDING          int64\nHITPARKEDCAR     object\ndtype: object"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns = ['ADDRTYPE','COLLISIONTYPE','JUNCTIONTYPE','WEATHER','ROADCOND','LIGHTCOND','HITPARKEDCAR','PEDROWNOTGRNT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[cat_columns] = data[cat_columns].apply(lambda x: x.astype('category').cat.codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "        ADDRTYPE  SEVERITYCODE  COLLISIONTYPE  JUNCTIONTYPE  UNDERINFL  \\\n199624         1             1              0             3          0   \n199625         1             1              7             4          0   \n199626         1             2              7             4          0   \n199627         2             1              8             1          0   \n199628         1             1              0             2          0   \n199629         1             2              5             4          0   \n\n        WEATHER  ROADCOND  LIGHTCOND  PEDROWNOTGRNT  SPEEDING  HITPARKEDCAR  \n199624        5         0          5              0         0             0  \n199625       11         7          5              0         0             0  \n199626        2         0          5              0         0             0  \n199627        2         0          5              0         0             0  \n199628        2         0          5              0         0             0  \n199629        2         0          5              0         0             0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ADDRTYPE</th>\n      <th>SEVERITYCODE</th>\n      <th>COLLISIONTYPE</th>\n      <th>JUNCTIONTYPE</th>\n      <th>UNDERINFL</th>\n      <th>WEATHER</th>\n      <th>ROADCOND</th>\n      <th>LIGHTCOND</th>\n      <th>PEDROWNOTGRNT</th>\n      <th>SPEEDING</th>\n      <th>HITPARKEDCAR</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>199624</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>5</td>\n      <td>0</td>\n      <td>5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>199625</th>\n      <td>1</td>\n      <td>1</td>\n      <td>7</td>\n      <td>4</td>\n      <td>0</td>\n      <td>11</td>\n      <td>7</td>\n      <td>5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>199626</th>\n      <td>1</td>\n      <td>2</td>\n      <td>7</td>\n      <td>4</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>199627</th>\n      <td>2</td>\n      <td>1</td>\n      <td>8</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>199628</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>199629</th>\n      <td>1</td>\n      <td>2</td>\n      <td>5</td>\n      <td>4</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "data.tail(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns=['SEVERITYCODE'],axis=1)\n",
    "Y = data['SEVERITYCODE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "        Features        Score\n1  COLLISIONTYPE  5670.606450\n2   JUNCTIONTYPE  5312.475739\n4        WEATHER  4875.036603\n5       ROADCOND  2745.880663\n9   HITPARKEDCAR  2092.978875\n0       ADDRTYPE  1363.455201\n3      UNDERINFL  1132.955601\n8       SPEEDING   892.711942\n6      LIGHTCOND   609.365801\n"
    }
   ],
   "source": [
    "#apply SelectKBest to extract top 5 best features for housing df\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
    "\n",
    "bestfeatures = SelectKBest(score_func=chi2, k=5)\n",
    "fit = bestfeatures.fit(X,Y)\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(X.columns)\n",
    "#concat two dataframes for better visualization \n",
    "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "featureScores.columns = ['Features','Score']  #naming the dataframe columns\n",
    "print(featureScores.nlargest(14,'Score'))  #print 5best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.30, stratify=Y,random_state=66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1    96239\n2    41089\n3     2169\n4      244\nName: SEVERITYCODE, dtype: int64"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1    41246\n2    17609\n3      929\n4      105\nName: SEVERITYCODE, dtype: int64"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "weights = class_weight.compute_class_weight('balanced', np.unique(y_train),y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([  0.36300512,   0.85023364,  16.10661595, 143.1772541 ])"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{1: 0.3630051226633693,\n 2: 0.8502336391735015,\n 3: 16.106615952051637,\n 4: 143.17725409836066}"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "class_weights = {}\n",
    "for i in range(4):\n",
    "    class_weights[i+1] = weights[i]\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_info_df_4_class = pd.DataFrame([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying different models having severity code 1,2,3,4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy on training set: 0.4841170451048726\nAccuracy on test set: 0.48144066523067675\nF1 score 0.554102110056551\n              precision    recall  f1-score   support\n\n           1       0.78      0.57      0.66     41246\n           2       0.41      0.30      0.34     17609\n           3       0.02      0.19      0.03       929\n           4       0.01      0.45      0.01       105\n\n    accuracy                           0.48     59889\n   macro avg       0.30      0.38      0.26     59889\nweighted avg       0.66      0.48      0.55     59889\n\n"
    }
   ],
   "source": [
    "\n",
    "lr1 = LogisticRegression(class_weight=class_weights,random_state = 0)\n",
    "lr1.fit(X_train, y_train)\n",
    "y_pred_lr1 = lr1.predict(X_test)\n",
    "\n",
    "\n",
    "print('Accuracy on training set:',lr1.score(X_train,y_train))\n",
    "print('Accuracy on test set:',lr1.score(X_test,y_test))\n",
    "print(\"F1 score\", f1_score(y_test, y_pred_lr1,average='weighted'))\n",
    "\n",
    "print (classification_report(y_test, y_pred_lr1))\n",
    "\n",
    "models_info_df_4_class = models_info_df_4_class.append([(\"Logistic Regression 4 class\",lr1.score(X_train,y_train), lr1.score(X_test,y_test), f1_score(y_test, y_pred_lr1,average='weighted'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy on training set: 0.5717219713613041\nAccuracy on test set: 0.5540416437075256\nF1 Score :  0.6098873717988984\n              precision    recall  f1-score   support\n\n           1       0.88      0.56      0.68     41246\n           2       0.41      0.56      0.48     17609\n           3       0.05      0.29      0.08       929\n           4       0.01      0.46      0.02       105\n\n    accuracy                           0.55     59889\n   macro avg       0.34      0.47      0.32     59889\nweighted avg       0.73      0.55      0.61     59889\n\n"
    }
   ],
   "source": [
    "\n",
    "rf_clf1 = RandomForestClassifier(n_estimators = 10, random_state = 0,class_weight=class_weights)\n",
    "rf_clf1.fit(X_train, y_train)\n",
    "y_pred_rfc1 = rf_clf1.predict(X_test)\n",
    "print('Accuracy on training set:',rf_clf1.score(X_train,y_train))\n",
    "print('Accuracy on test set:',rf_clf1.score(X_test,y_test))\n",
    "print(\"F1 Score : \",f1_score(y_test,y_pred_rfc1, average='weighted'))\n",
    "print(classification_report(y_test, y_pred_rfc1))\n",
    "models_info_df_4_class = models_info_df_4_class.append([(\"RandomForestClassifier 4 class\",rf_clf1.score(X_train,y_train), rf_clf1.score(X_test,y_test), f1_score(y_test,y_pred_rfc1, average='weighted') )])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy on training set 0.565782411747447\nAccuracy on test set: 0.5461937918482526\nF1 Score :  0.6054459686369363\n              precision    recall  f1-score   support\n\n           1       0.88      0.55      0.68     41246\n           2       0.41      0.55      0.47     17609\n           3       0.05      0.30      0.08       929\n           4       0.01      0.51      0.02       105\n\n    accuracy                           0.55     59889\n   macro avg       0.34      0.48      0.31     59889\nweighted avg       0.73      0.55      0.61     59889\n\n"
    }
   ],
   "source": [
    "\n",
    "dtc1 = DecisionTreeClassifier(class_weight=class_weights)\n",
    "dtc1.fit(X_train, y_train)\n",
    "y_pred_dtc1 = dtc1.predict(X_test)\n",
    "print('Accuracy on training set',dtc1.score(X_train, y_train))\n",
    "print('Accuracy on test set:',dtc1.score(X_test, y_test))\n",
    "print(\"F1 Score : \",f1_score(y_test,y_pred_dtc1, average='weighted'))\n",
    "print(classification_report(y_test, y_pred_dtc1))\n",
    "models_info_df_4_class = models_info_df_4_class.append([(\"DecisionTreeClassifier 4 class\",dtc1.score(X_train, y_train), dtc1.score(X_test, y_test), f1_score(y_test,y_pred_dtc1, average='weighted') )])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[09:18:39] WARNING: ../src/learner.cc:516: \nParameters: { class_weight } might not be used.\n\n  This may not be accurate due to some parameters are only used in language bindings but\n  passed down to XGBoost core.  Or some parameters are not used but slip through this\n  verification. Please open an issue if you find above cases.\n\n\nAccuracy score on train_data:  0.7397971962416184\nAccuracy score on test_data:  0.7345756315850991\nF1 score 0.6807308313247457\n              precision    recall  f1-score   support\n\n           1       0.74      0.98      0.84     41246\n           2       0.72      0.21      0.33     17609\n           3       0.38      0.00      0.01       929\n           4       0.00      0.00      0.00       105\n\n    accuracy                           0.73     59889\n   macro avg       0.46      0.30      0.29     59889\nweighted avg       0.72      0.73      0.67     59889\n\n"
    }
   ],
   "source": [
    "\n",
    "xgb_clf1= XGBClassifier(class_weight=class_weights)\n",
    "xgb_clf1.fit(X_train, y_train)\n",
    "\n",
    "y_pred_xgb1 = xgb_clf1.predict(X_test)\n",
    "\n",
    "print('Accuracy score on train_data: ', accuracy_score(y_true = y_train, y_pred = xgb_clf1.predict(X_train).round()))\n",
    "print('Accuracy score on test_data: ', accuracy_score(y_true = y_test, y_pred = xgb_clf1.predict(X_test).round()))\n",
    "print(\"F1 score\", f1_score(y_true = y_train, y_pred = xgb_clf1.predict(X_train).round(), average='weighted'))\n",
    "print(classification_report(y_test, y_pred_xgb1))\n",
    "models_info_df_4_class = models_info_df_4_class.append([(\"XGBClassifier 4 class\",accuracy_score(y_true = y_train, y_pred = xgb_clf1.predict(X_train).round()),accuracy_score(y_true = y_test, y_pred = xgb_clf1.predict(X_test).round()), f1_score(y_true = y_train, y_pred = xgb_clf1.predict(X_train).round(), average='weighted') )])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy on training set: 0.7065857550754611\nAccuracy on test set: 0.6991100202040441\nF1 Score :  0.685062395535323\n              precision    recall  f1-score   support\n\n           1       0.77      0.83      0.80     41246\n           2       0.50      0.43      0.46     17609\n           3       0.21      0.01      0.01       929\n           4       0.00      0.00      0.00       105\n\n    accuracy                           0.70     59889\n   macro avg       0.37      0.32      0.32     59889\nweighted avg       0.68      0.70      0.69     59889\n\n"
    }
   ],
   "source": [
    "\n",
    "knn_clf1 = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)  \n",
    "#You can change these hyperparameters like metric etc.\n",
    "knn_clf1.fit(X_train, y_train)\n",
    "y_pred_knn1 = knn_clf1.predict(X_test)\n",
    "print('Accuracy on training set:',knn_clf1.score(X_train,y_train))\n",
    "print('Accuracy on test set:',knn_clf1.score(X_test,y_test))\n",
    "print(\"F1 Score : \",f1_score(y_test,y_pred_knn1, average='weighted'))\n",
    "print(classification_report(y_test, y_pred_knn1))\n",
    "models_info_df_4_class = models_info_df_4_class.append([(\"KNeighborsClassifier 4 class\",knn_clf1.score(X_train,y_train), knn_clf1.score(X_test,y_test),f1_score(y_test,y_pred_knn1, average='weighted'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying Different models having severity code 1,2 = 1 and 3,4 = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv(\"data_cleaned.csv\")\n",
    "\n",
    "data2.drop(columns=['INCDTTM','year','day','month'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns = ['ADDRTYPE','COLLISIONTYPE','JUNCTIONTYPE','WEATHER','ROADCOND','LIGHTCOND','HITPARKEDCAR','PEDROWNOTGRNT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2[cat_columns] = data2[cat_columns].apply(lambda x: x.astype('category').cat.codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1    196183\n2      3447\nName: SEVERITYCODE, dtype: int64"
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "# # Ordering 1 & 2 as 1 and 3 & 4 as 2\n",
    "data2['SEVERITYCODE'].replace(2,1,inplace=True)\n",
    "data2['SEVERITYCODE'].replace(3,2,inplace=True)\n",
    "data2['SEVERITYCODE'].replace(4,2,inplace=True)\n",
    "data2['SEVERITYCODE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = data2.drop(columns=['SEVERITYCODE'],axis=1)\n",
    "Y2 = data2['SEVERITYCODE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, Y2, test_size=0.30, stratify=Y2,random_state=66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "weights2 = class_weight.compute_class_weight('balanced', np.unique(y2_train),y2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([ 0.50878554, 28.95586407])"
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "weights2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{1: 0.5087855353605966, 2: 28.955864069622876}"
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "class_weights2 = {}\n",
    "for i in range(2):\n",
    "    class_weights2[i+1] = weights2[i]\n",
    "class_weights2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_info_df_2_class = pd.DataFrame([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy on training set: 0.6356044396419089\nAccuracy on test set: 0.6349747032009218\nF1 score 0.761400059381787\n              precision    recall  f1-score   support\n\n           1       0.99      0.64      0.77     58855\n           2       0.03      0.61      0.05      1034\n\n    accuracy                           0.63     59889\n   macro avg       0.51      0.62      0.41     59889\nweighted avg       0.97      0.63      0.76     59889\n\n"
    }
   ],
   "source": [
    "\n",
    "lr2 = LogisticRegression(class_weight=class_weights2,random_state = 0)\n",
    "lr2.fit(X2_train, y2_train)\n",
    "y_pred_lr2 = lr2.predict(X2_test)\n",
    "\n",
    "\n",
    "print('Accuracy on training set:',lr2.score(X2_train,y2_train))\n",
    "print('Accuracy on test set:',lr2.score(X2_test,y2_test))\n",
    "print(\"F1 score\", f1_score(y2_test, y_pred_lr2, average='weighted'))\n",
    "\n",
    "print (classification_report(y2_test, y_pred_lr2))\n",
    "\n",
    "models_info_df_2_class = models_info_df_2_class.append([(\"Logistic Regression 2 class\",lr2.score(X2_train,y2_train), lr2.score(X2_test,y2_test), f1_score(y2_test, y_pred_lr2,average='weighted'))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Clasifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy on training set: 0.9833477647934393\nAccuracy on test set: 0.9823172869809147\nF1 Score :  0.9741292828086912\n              precision    recall  f1-score   support\n\n           1       0.98      1.00      0.99     58855\n           2       0.14      0.00      0.01      1034\n\n    accuracy                           0.98     59889\n   macro avg       0.56      0.50      0.50     59889\nweighted avg       0.97      0.98      0.97     59889\n\n"
    }
   ],
   "source": [
    "\n",
    "rf_clf2 = RandomForestClassifier(n_estimators = 10, random_state = 0)\n",
    "rf_clf2.fit(X2_train, y2_train)\n",
    "y_pred_rfc2 = rf_clf2.predict(X2_test)\n",
    "print('Accuracy on training set:',rf_clf2.score(X2_train,y2_train))\n",
    "print('Accuracy on test set:',rf_clf2.score(X2_test,y2_test))\n",
    "print(\"F1 Score : \",f1_score(y2_test,y_pred_rfc2, average='weighted'))\n",
    "print(classification_report(y2_test, y_pred_rfc2))\n",
    "models_info_df_2_class = models_info_df_2_class.append([(\"RandomForestClassifier 2 class\",rf_clf2.score(X2_train,y2_train), rf_clf2.score(X2_test,y2_test), f1_score(y2_test,y_pred_rfc2, average='weighted') )])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy on training set 0.7985058071718393\nAccuracy on test set: 0.7941358179298369\nF1 Score :  0.870235073849254\n              precision    recall  f1-score   support\n\n           1       0.99      0.80      0.88     58855\n           2       0.05      0.60      0.09      1034\n\n    accuracy                           0.79     59889\n   macro avg       0.52      0.70      0.49     59889\nweighted avg       0.98      0.79      0.87     59889\n\n"
    }
   ],
   "source": [
    "\n",
    "dtc2 = DecisionTreeClassifier(class_weight=class_weights2)\n",
    "dtc2.fit(X2_train, y2_train)\n",
    "y_pred_dtc2 = dtc2.predict(X2_test)\n",
    "print('Accuracy on training set',dtc2.score(X2_train, y2_train))\n",
    "print('Accuracy on test set:',dtc2.score(X2_test, y2_test))\n",
    "print(\"F1 Score : \",f1_score(y2_test,y_pred_dtc2, average='weighted'))\n",
    "print(classification_report(y2_test, y_pred_dtc2))\n",
    "models_info_df_2_class = models_info_df_2_class.append([(\"DecisionTreeClassifier 2 class\",dtc2.score(X2_train, y2_train), dtc2.score(X2_test, y2_test), f1_score(y2_test,y_pred_dtc2, average='weighted') )])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy score on train_data:  0.9828683063667786\nAccuracy score on test_data:  0.9826345405667151\nF1 score 0.974579618482291\n              precision    recall  f1-score   support\n\n           1       0.98      1.00      0.99     58855\n           2       0.25      0.00      0.01      1034\n\n    accuracy                           0.98     59889\n   macro avg       0.62      0.50      0.50     59889\nweighted avg       0.97      0.98      0.97     59889\n\n"
    }
   ],
   "source": [
    "\n",
    "xgb_clf2 = XGBClassifier()\n",
    "xgb_clf2.fit(X2_train, y2_train)\n",
    "\n",
    "y_pred_xgb2 = xgb_clf2.predict(X2_test)\n",
    "\n",
    "print('Accuracy score on train_data: ', accuracy_score(y_true = y2_train, y_pred = xgb_clf2.predict(X2_train).round()))\n",
    "print('Accuracy score on test_data: ', accuracy_score(y_true = y2_test, y_pred = xgb_clf2.predict(X2_test).round()))\n",
    "print(\"F1 score\", f1_score(y_true = y2_train, y_pred = xgb_clf2.predict(X2_train).round(), average='weighted'))\n",
    "print(classification_report(y2_test, y_pred_xgb2))\n",
    "models_info_df_2_class = models_info_df_2_class.append([(\"XGBClassifier 2 class\",accuracy_score(y_true = y2_train, y_pred = xgb_clf2.predict(X2_train).round()),accuracy_score(y_true = y2_test, y_pred = xgb_clf2.predict(X2_test).round()), f1_score(y_true = y2_train, y_pred = xgb_clf2.predict(X2_train).round(), average='weighted') )])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy on training set: 0.9816303017725649\nAccuracy on test set: 0.9817161749236086\nF1 Score :  0.9743891439677576\n              precision    recall  f1-score   support\n\n           1       0.98      1.00      0.99     58855\n           2       0.22      0.02      0.04      1034\n\n    accuracy                           0.98     59889\n   macro avg       0.60      0.51      0.52     59889\nweighted avg       0.97      0.98      0.97     59889\n\n"
    }
   ],
   "source": [
    "\n",
    "knn_clf2 = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)  \n",
    "#You can change these hyperparameters like metric etc.\n",
    "knn_clf2.fit(X2_train, y2_train)\n",
    "y_pred_knn2 = knn_clf2.predict(X2_test)\n",
    "print('Accuracy on training set:',knn_clf2.score(X2_train,y2_train))\n",
    "print('Accuracy on test set:',knn_clf2.score(X2_test,y2_test))\n",
    "print(\"F1 Score : \",f1_score(y2_test,y_pred_knn2, average='weighted'))\n",
    "print(classification_report(y2_test, y_pred_knn2))\n",
    "models_info_df_2_class = models_info_df_2_class.append([(\"KNeighborsClassifier 2 class\",knn_clf2.score(X2_train,y2_train), knn_clf2.score(X2_test,y2_test),f1_score(y2_test,y_pred_knn2, average='weighted'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                           models  Training Accuracy  Test Accuracy  f1 score\n0     Logistic Regression 4 class           0.484117       0.481441  0.554102\n0  RandomForestClassifier 4 class           0.571722       0.554042  0.609887\n0  DecisionTreeClassifier 4 class           0.565782       0.546194  0.605446\n0           XGBClassifier 4 class           0.739797       0.734576  0.680731\n0    KNeighborsClassifier 4 class           0.706586       0.699110  0.685062",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>models</th>\n      <th>Training Accuracy</th>\n      <th>Test Accuracy</th>\n      <th>f1 score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Logistic Regression 4 class</td>\n      <td>0.484117</td>\n      <td>0.481441</td>\n      <td>0.554102</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>RandomForestClassifier 4 class</td>\n      <td>0.571722</td>\n      <td>0.554042</td>\n      <td>0.609887</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>DecisionTreeClassifier 4 class</td>\n      <td>0.565782</td>\n      <td>0.546194</td>\n      <td>0.605446</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>XGBClassifier 4 class</td>\n      <td>0.739797</td>\n      <td>0.734576</td>\n      <td>0.680731</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>KNeighborsClassifier 4 class</td>\n      <td>0.706586</td>\n      <td>0.699110</td>\n      <td>0.685062</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "models_info_df_4_class.columns = ['models','Training Accuracy','Test Accuracy', 'f1 score']\n",
    "models_info_df_4_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                           models  Training Accuracy  Test Accuracy  f1 score\n0     Logistic Regression 2 class           0.635604       0.634975  0.761400\n0  RandomForestClassifier 2 class           0.983348       0.982317  0.974129\n0  DecisionTreeClassifier 2 class           0.798506       0.794136  0.870235\n0           XGBClassifier 2 class           0.982868       0.982635  0.974580\n0    KNeighborsClassifier 2 class           0.981630       0.981716  0.974389",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>models</th>\n      <th>Training Accuracy</th>\n      <th>Test Accuracy</th>\n      <th>f1 score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Logistic Regression 2 class</td>\n      <td>0.635604</td>\n      <td>0.634975</td>\n      <td>0.761400</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>RandomForestClassifier 2 class</td>\n      <td>0.983348</td>\n      <td>0.982317</td>\n      <td>0.974129</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>DecisionTreeClassifier 2 class</td>\n      <td>0.798506</td>\n      <td>0.794136</td>\n      <td>0.870235</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>XGBClassifier 2 class</td>\n      <td>0.982868</td>\n      <td>0.982635</td>\n      <td>0.974580</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>KNeighborsClassifier 2 class</td>\n      <td>0.981630</td>\n      <td>0.981716</td>\n      <td>0.974389</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "models_info_df_2_class.columns = ['models','Training Accuracy','Test Accuracy', 'f1 score']\n",
    "models_info_df_2_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}